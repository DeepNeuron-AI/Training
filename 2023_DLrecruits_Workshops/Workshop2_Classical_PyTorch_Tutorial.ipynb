{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGTnInyEharX"
      },
      "source": [
        "# Before you start\n",
        "1. **Don't edit this file, make a copy first:**\n",
        "  * Click on File -> Save a copy in Drive\n",
        "\n",
        "2. Also do the following:\n",
        "  * Click on Runtime -> Change runtime type -> Make sure hardware accelerator is set to GPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cknZbgNdR6IM"
      },
      "source": [
        "# An Overview Before We Begin\n",
        "Here's a couple of important concepts to note down before we start:\n",
        "- There is no one particular methodology that works best in all scenarios. This includes everything from model architecture, learning rate, loss function, and optimizer. \n",
        "- Like any other engineering project, validation of what we have built is just as important as building it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZueWb5ZRDeyT"
      },
      "source": [
        "# Library Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DlJ_aMz20rxE"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from torchvision import datasets, transforms, models\n",
        "\n",
        "from tqdm.notebook import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hepIxB_VKggb"
      },
      "source": [
        "# Defining Path Variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hO3kSpVxKiqC"
      },
      "source": [
        "train_path = 'data/train'\n",
        "valid_path = 'data/valid'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zszDeZ7CJ0Z3"
      },
      "source": [
        "# Creating DataLoader (DataBunch in FastAI)\n",
        "- Main difference between FastAI and PyTorch here is that there are 2 steps to creating a DataLoader in PyTorch.\n",
        "  1. Creating a Dataset. This bundles the data in a way that the model can understand.\n",
        "  2. Creating a DataLoader. This tells the model how to receive the images. Including batch_size, num_workers, shuffle configurations, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1LivjuALGLi"
      },
      "source": [
        "## [Instantiating Transforms](https://https://pytorch.org/docs/stable/torchvision/transforms.html)\n",
        "- Transforms are the same in FastAI .transform() function\n",
        "  - the transforms.Normalize([...]) function basically changes the data slightly according to the average RGB weights in the ImageNet dataset. \n",
        "  - This may seem a bit strange to you, why do we do this? Turns out, normalizing the data before training results in noticeable performance gains and reduction in training time. \n",
        "  - So why does normalizing data have such performance boosts in training? That's because by itself, the RGB values of the raw data have differing ranges. the blue pixel may have a range of of 0->125 while the red pixel may have a range of 120->245. This different range often causes headaches for the optimizer and it takes the gradient descent to converge much slowly as it has to cater to the differing conditions of both the red and blue pixel.\n",
        "  - What batch normalization does is that it makes the RGB ranges somewhat similar, so the optimizer doesn't have such a hard time trying to cater for all the different ranges, and thus gradient descent covnerges faster. \n",
        "  - More information here https://medium.com/@urvashilluniya/why-data-normalization-is-necessary-for-machine-learning-models-681b65a05029\n",
        "\n",
        "Now, in the below block of code, come up with a set of tranforms for the training and validation datasets that you think might be suitable. Try using transforms such as rotation, flipping, normalizing, etc.\n",
        "\n",
        "Find the documentation for the transforms here : https://pytorch.org/vision/stable/transforms.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXzCC_fKJzVT"
      },
      "source": [
        "# Define transforms for the training and validation set\n",
        "training_transforms = transforms.Compose([# Insert random rotation, 30 degrees\n",
        "                                          # Insert random horizontal flip\n",
        "                                          # Convert to tensor\n",
        "                                          transforms.Normalize([0.485, 0.456, 0.406], \n",
        "                                                               [0.229, 0.224, 0.225])])\n",
        "\n",
        "validation_transforms = transforms.Compose([# Convert to tensor\n",
        "                                            transforms.Normalize([0.485, 0.456, 0.406], \n",
        "                                                                 [0.229, 0.224, 0.225])])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQ9qAGv1LgLC"
      },
      "source": [
        "## [Torchvision datasets](https://pytorch.org/docs/stable/torchvision/datasets.html)\n",
        "There are a number of ways to create a dataset, for example: \n",
        "- Use an available Torchvision dataset\n",
        "    - We're using one below called CIFAR10 which we used in the first training session\n",
        "- Use ImageFolder to create a dataset from folders\n",
        "- Write your own dataset as a subclass of torch.utils.data.Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_7ENLBwKYKO"
      },
      "source": [
        "training_dataset = datasets.CIFAR10(train_path, train=True, transform=training_transforms, download=True)\n",
        "validation_dataset = datasets.CIFAR10(valid_path, train=False, transform=validation_transforms, download=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njZjCupaUJQN"
      },
      "source": [
        "## [Instantiating DataLoader](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html)\n",
        "- This tells the model *how* how to receive the data; with the dataset as an input, along with batch size and shuffle as args \n",
        "\n",
        "Now code up data loaders for the training and validation datasets as per the followig specs : \n",
        "\n",
        "- In the training_loader, we're telling it batch_size = 32, and we want to shuffle the dataloader after each epoch. \n",
        "- In the validation_loader, we also take batch_size = 32 but we DON'T want to shuffle the dataloader. This is because we want to be testing on the data in the same order to make sure the model really is improving and didn't hit a fluke ordering of the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFxxs8XJULmT"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "training_loader = \n",
        "validation_loader = "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3t0Ga6RYLOA4"
      },
      "source": [
        "# Check what classes are in our dataset\n",
        "\n",
        "training_dataset.classes, validation_dataset.classes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NwqQ0DMFlBC"
      },
      "source": [
        "# Instantiating ResNet18\n",
        "- Unlike FastAI, when a model is downloaded, you need to reconfigure the 'classification' layer as the pretrained model was trained for ImageNet, hence it comes ready to classify for many classes (we only need it to classify 10 classes)\n",
        "- In addition, downloaded models from PyTorch come unfrozen, which means we need to 'freeze' the entire network except for the classification layer so we can perform the first batch of training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXc2SykWNm8J"
      },
      "source": [
        "model = models.resnet18(pretrained=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMqHGCoyGb0d"
      },
      "source": [
        "## Freezing The Model\n",
        "- *%%capture* is colab syntax, it essentially stops the cell from printing out any logs. This is purely for *aesthetic* purposes.\n",
        "- We're looping through all the parameters in the model, and setting requires_grad = False. Which 'freezes' the entire model.\n",
        "  - requires_grad stands for 'requires gradient'. When requires_grad is False, it's weights does not get updated and hence it is 'frozen'."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oy-Sty6TXRWt"
      },
      "source": [
        "%%capture\n",
        "for param in model.parameters():\n",
        "  param.requires_grad = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SiySi5PyGzKF"
      },
      "source": [
        "## Reconfiguring The Classification Layer\n",
        "- model.fc = the last layer of the network\n",
        "- model.fc.in_features = the features going into the last layer\n",
        "- But we know that the pretrained Resnet18 is designed for dozens of classes, but we only need it to classify 10 classes, so we're going to have to replace the last layer. \n",
        "  - To do so, we use nn.Linear(out_ftrs, 10). \n",
        "  - This way, we keep the same numnber of features going into the last layer, but only change the number of features going out, which in this case is 10. \n",
        "  - We then reinsert it to the model using model.fc = nn.Linear(out_ftrs, 10)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print last layer of the model\n",
        "model.fc"
      ],
      "metadata": {
        "id": "pw-6SGpx0vKJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-X4os5mPCMj"
      },
      "source": [
        "# Redefine final linear layer, such that output 10 classes\n",
        "\n",
        "out_ftrs = # Number of features going INTO the last layer\n",
        "model.fc = nn.Linear() # Redefine last linear layer of network to output 10 classes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jKFQfhkhGf1"
      },
      "source": [
        "# The Training Function\n",
        "- Just like the fit() function in fast.ai we're going to need a function that trains out model\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "Everytime we run through a 'batch' of data we need to do a few things\n",
        "1. Clear the gradients from the previous loop  \n",
        "2. Perform a forward pass (put the input through the model once)\n",
        "3. Calculate the loss\n",
        "4. Back propogate the loss\n",
        "5. Update the parameter weights by taking a step with the optimiser"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gwvXsvTnhEKd"
      },
      "source": [
        "# Function for the training \n",
        "\n",
        "def train(model, train_loader, loss_fn, optimizer, device):\n",
        "    model.train() # puts the model in training mode\n",
        "    running_loss = 0\n",
        "    with tqdm(total=len(train_loader)) as pbar:\n",
        "        for i, data in enumerate(train_loader, 0): # loops through training data\n",
        "            inputs, labels = data # separate inputs and labels (outputs)\n",
        "            inputs, labels = inputs.to(device), labels.to(device) # puts the data on the GPU\n",
        "\n",
        "            # forward + backward + optimize                                          \n",
        "            optimizer.zero_grad() # clear the gradients in model parameters\n",
        "            outputs = model(inputs) # forward pass and get predictions\n",
        "            loss = loss_fn(outputs, labels) # calculate loss\n",
        "            loss.backward() # calculates gradient w.r.t to loss for all parameters in model that have requires_grad=True\n",
        "            optimizer.step() # iterate over all parameters in the model with requires_grad=True and update their weights.\n",
        "\n",
        "            running_loss += loss.item() # sum total loss in current epoch for print later\n",
        "\n",
        "            pbar.update(1) #increment our progress bar\n",
        "\n",
        "    return running_loss/len(train_loader) # returns the total training loss for the epoch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPD3ezm9OZ8t"
      },
      "source": [
        "# The Validation Function\n",
        "- A validation function is essential in any model training, because it helps you validate how well your model is performing on the validation dataset. \n",
        "\n",
        "Note: the validation function validates the model performance by passing the entire validation set through the model ONCE. Also note that we cacluate the loss but don't propogate it back or update any weights!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2QY58AvqJDI"
      },
      "source": [
        "# Function for the validation pass\n",
        "\n",
        "def validation(model, val_loader, loss_fn, device):\n",
        "    model.eval() # puts the model in validation mode\n",
        "    running_loss = 0\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    \n",
        "    with torch.no_grad(): # save memory by not saving gradients which we don't need \n",
        "        with tqdm(total=len(val_loader)) as pbar:\n",
        "            for images, labels in iter(val_loader):\n",
        "                images, labels = images.to(device), labels.to(device) # put the data on the GPU\n",
        "                outputs = model(images) # passes image to the model, and gets a ouput which is the class probability prediction\n",
        "\n",
        "                val_loss = loss_fn(outputs, labels) # calculates val_loss from model predictions and true labels\n",
        "                running_loss += val_loss.item()\n",
        "                _, predicted = torch.max(outputs, 1) # turns class probability predictions to class labels\n",
        "                total += labels.size(0) # sums the number of predictions\n",
        "                correct += (predicted == labels).sum().item() # sums the number of correct predictions\n",
        "        \n",
        "                pbar.update(1)\n",
        "\n",
        "        return running_loss/len(val_loader), correct/total # return loss value, accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cb8hexwc-8ES"
      },
      "source": [
        "#Things to note about our training and validation functions\n",
        "\n",
        "## What's the difference between `model.train()` and `model.eval()`? \n",
        "These two are extremely important to your training and validation loops. `model.eval()` takes away some layers that should only be used during training such as dropout and batch normalisation. It's important to always use `model.train()` when training and `model.eval()` when evaluating. \n",
        "\n",
        "## Why do we need torch.no_grad()?\n",
        "Running `with torch.no_grad()` means that we don't want gradients which is what happens during validation or testing, we don't need to update any gradients so we don't need to record them. Running this means that we optimize our code to not do things it doesn't need to. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLUTOwwH4-9T"
      },
      "source": [
        "# Setting Up Training\n",
        "- When training models, it is substantially faster to train on NVIDIA GPU's, beacuse they offer a parallel computing platform called [cuda](https://developer.nvidia.com/cuda-zone) (cudnn is the API package to interface with cuda) that speeds up these computations exponentially. \n",
        "- So here we check if cuda is available with cuda.is_available().\n",
        "  - Following which, we send the model to the cuda device so the computation can be done on the GPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "On6Xdb9TqNOF"
      },
      "source": [
        "%%capture\n",
        "import torch.backends.cudnn as cudnn\n",
        "torch.cuda.empty_cache()\n",
        "cudnn.benchmark = True  # Optimise for hardware\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device) # send model to GPU"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4pZVkP5QiwM"
      },
      "source": [
        "# Loss Function & Optimizers\n",
        "- The **Loss Function** calculates how 'far' the model's class probability predictions are to the actual labels.\n",
        "  - Notice how I'm saying \"how far the model's class prob predictions are to the actual labels\" instead of \"how innacurate the model is\", that's because accurate/inaccurate is the percentage of correctly or incorrectly predicted labels. This may sound the same to you but just keep this in mind, it will all make sense in due time.\n",
        "  - CrossEntropyLoss is a way of calculating the loss of a model, other loss functions include Kullback Leibler Divergence Loss, Sparse Multiclass Cross-Entropy Loss, and much more. \n",
        "\n",
        "- **The Optimizer** is a way of updating the weights of the model to minimize loss. In other words, the optimizer is the part of deep learning that helps a model 'learn'.\n",
        "- In this case we're using the Adam optimizer, this is purely by random choice as no particular optimizer can be said to be superior to the other. There's an important concept in deep learning called \"no free lunch\", which means there isn't a particular methodology that will achieve the best outcome for all scenarios, what it comes down to is experimentation. \n",
        "  - a lr of 0.001 is also chosen, this is usually a good learning rate start from with the Adam optimizer, however to get a more optimum learning rate, experimentation would need to be done. (The Pytorch documentation includes defaults for each different optimizer)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IoIZ_7nsqKbt"
      },
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xedFUaJCMNMR"
      },
      "source": [
        "# Let The Training Begin! Part 1\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lL8voSz-4fKi"
      },
      "source": [
        "total_epoch = 10\n",
        "for epoch in range(total_epoch): # loops through number of epochs\n",
        "  train_loss = train(model, training_loader, loss_fn, optimizer, device)  # train the model for one epoch\n",
        "  val_loss, accuracy = validation(model, validation_loader, loss_fn, device) # after training for one epoch, run the validation() function to see how the model is doing on the validation dataset\n",
        "  print(\"Epoch: {}/{}, Training Loss: {}, Val Loss: {}, Val Accuracy: {}\".format(epoch+1, total_epoch, train_loss, val_loss, accuracy))\n",
        "  print('-' * 20)\n",
        "\n",
        "print(\"Finished Training\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-J2_a9Gbggo"
      },
      "source": [
        "# as we do in FastAI, we save the model so we can come back later to it if need be\n",
        "torch.save(model.state_dict(), 'stage-1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbNvgW6rOGLg"
      },
      "source": [
        "# Let The Training Begin! Part 2\n",
        "- Remember in FastAI we called the 'unfreeze()' function after the first batch of training? The below cell does the exact same thing, what it does is it allows the rest of the model to be optimized for this specific task.\n",
        "- The cell after is exactly the same as the training of the model in 'let The Training Begin! Part 1', just that we're retraining for 2 epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7rN2kYE2zLW"
      },
      "source": [
        "%%capture\n",
        "for param in model.parameters():\n",
        "  param.requires_grad = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hwo7c3hz22lr"
      },
      "source": [
        "total_epoch = 10\n",
        "for epoch in range(total_epoch): # loops through number of epochs\n",
        "  train_loss = train(model, training_loader, loss_fn, optimizer, device) # train the model for one epoch\n",
        "  val_loss, accuracy = validation(model, validation_loader, loss_fn, device) # after training for one epoch, run the validation() function to see how the model is doing on the validation dataset\n",
        "  print(\"Epoch: {}/{}, Training Loss: {}, Val Loss: {}, Val Accuracy: {}\".format(epoch+1, total_epoch, train_loss, val_loss, accuracy))\n",
        "  print('-' * 20)\n",
        "\n",
        "print(\"Finished Training\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZikgH7ANmcXA"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}