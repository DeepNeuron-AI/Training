{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vThOmJ2yn5YY"
      },
      "source": [
        "# Workshop 4 - Resnets\n",
        "\n",
        "**Remember to save a copy to your own drive!!**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNpApSiScVEY"
      },
      "source": [
        "# Import libraries\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch import optim\n",
        "\n",
        "from fastai.vision.all import untar_data, URLs\n",
        "\n",
        "import os, sys\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHwxeS3rc16z"
      },
      "source": [
        "# Get device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TL4huf6CavvI"
      },
      "source": [
        "# Dataset and Transforms\n",
        "Fastai has some [interesting datasets](https://course.fast.ai/datasets) that we can download and use for practice or for benchmarking. Today we'll be working with ImageWoof which is a subset of 10 classes from ImageNet that are all dog breeds."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-LxwdbKfc3PK"
      },
      "source": [
        "# Download the dataset and store the path\n",
        "# ~1min to download\n",
        "path = untar_data(URLs.IMAGEWOOF)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TzKcOpI-avvJ"
      },
      "source": [
        "# If you haven't seen python path objects, they're just a simpler way to work with addresses.\n",
        "print(\"Type:\", type(path))\n",
        "print(\"Path\", path)\n",
        "print(path/\"some\"/\"directory\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6aTQKa3avvL"
      },
      "source": [
        "# Plot the data to get an idea of our samples\n",
        "size = 3\n",
        "num = 10\n",
        "f, axs = plt.subplots(1, num, figsize=(size * num, size))\n",
        "\n",
        "# Get all classnames\n",
        "classnames = os.listdir(path/\"train\")\n",
        "\n",
        "\n",
        "for i, classname in enumerate(classnames):\n",
        "    # Get image path\n",
        "    filename = os.listdir(path/\"train\"/classname)[0]\n",
        "    image_path = path/\"train\"/classname/filename\n",
        "    \n",
        "    # Plot image\n",
        "    plt.subplot(1, num, i + 1)\n",
        "    img = Image.open(image_path)\n",
        "    plt.axis('off')\n",
        "    plt.imshow(img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Af7a-cwlavvM"
      },
      "source": [
        "The dataset is composed of coloured images, all with different sizes. We will need to use transforms to standardise the input size for our model. Similarly, we will also want to convert our inputs to a PyTorch tensor and to normalise it, so that our model can better handle the inputs. We can also use augmentations to stretch our data further, changing the inputs as we load them from memory.\n",
        "\n",
        "The list of PyTorch transforms is available on their [documentation](https://pytorch.org/docs/stable/torchvision/transforms.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4epEx7hVfthb"
      },
      "source": [
        "# Define transforms for the training set\n",
        "\n",
        "# Resize the image to a 256x216 image\n",
        "# Add a random horizontal flip\n",
        "# Randomly crop the data to make it square\n",
        "# Transform the image to a tensor\n",
        "# Normalise the data with imagenet statistics (means: 0.485, 0.456, 0.406, std: 0.229, 0.224, 0.225)\n",
        "train_trans = transforms.Compose([transforms.Resize(256, interpolation=2),\n",
        "                                  transforms.RandomHorizontalFlip(),\n",
        "                                  transforms.RandomRotation(20),\n",
        "                                  transforms.RandomCrop(256, pad_if_needed=True, padding_mode=\"reflect\"),\n",
        "                                  transforms.ToTensor(),\n",
        "                                  transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "                                  ])\n",
        "\n",
        "# Copy the training transforms, but only include the transforms that aren't augmentations\n",
        "valid_trans = transforms.Compose([transforms.Resize(256, interpolation=2),\n",
        "                                  transforms.CenterCrop(256),\n",
        "                                  transforms.ToTensor(),\n",
        "                                  transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "                                  ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Custom dataset creation\n",
        "\n",
        "Sometimes our data won't fit into a nice structure that can be easily loaded by a premade function. In this case we need to write a custom dataloader.\n",
        "\n",
        "In general we will need 3 methods:\n",
        "\n",
        "1. `__init__(self, ...):` \n",
        "Here we can set up anything we need to use to load datapoints. Common things include file paths, class lists etc.\n",
        "\n",
        "2. `__len__(self):`\n",
        "This return the size of the full dataset, so other functions know how much data can be called in total.\n",
        "\n",
        "3. `__getitem__(self, idx):`\n",
        "This loads a single datapoint for us. We can't keep the entire dataset in memory at once, so instead we only load data when we need it. Ususally we'll return an datapoint (an image here) and it's label (here a number corresponding to it's class) as torch tensors."
      ],
      "metadata": {
        "id": "hh7C-XYENw9y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDogDataset(Dataset):\n",
        "    def __init__(self, data_path, mode, tf):\n",
        "        self.path = data_path/mode # This is the path to the data              \n",
        "        # arent RGB\n",
        "        self.datapoints_clean = []\n",
        "        for d in self.datapoints_unclean:\n",
        "            img = Image.open(self.path/d)\n",
        "            if transforms.functional.to_tensor(img).shape[0] == 3:\n",
        "                self.datapoints_clean.append(d)\n",
        "        \n",
        "        # (Note that this is a really bad way of fixing this that i'm only doing\n",
        "        # because I don't have time to code up a better solution. If you run into\n",
        "        # this with your data, it's way better to write a separate script to \n",
        "        # completely clean your data files directly)\n",
        "\n",
        "    def __len__(self):\n",
        "        # datapoints_clean has all the datapoints to use, so it's length\n",
        "        # is the size of the dataset\n",
        "        return len(self.datapoints_clean)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Get the image class as a number\n",
        "        dp_class = self.datapoints_clean[idx].split('/')[0]\n",
        "        img_label = torch.tensor(self.classes.index(dp_class))\n",
        "\n",
        "        # Load the image\n",
        "        img_pth = self.path/self.datapoints_clean[idx]\n",
        "        img = Image.open(img_pth)\n",
        "        \n",
        "        # Apply transforms\n",
        "        transformed_img = self.tf(img)\n",
        "\n",
        "        return transformed_img, img_label"
      ],
      "metadata": {
        "id": "D41qXFxXNwfm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_ZnYcOUc9kV"
      },
      "source": [
        "# Create the train and validation datasets\n",
        "# The train / validation split is predetermined in this dataset and divided by folder\n",
        "\n",
        "# This will use an inbuilt torch method to create the dataset.\n",
        "# This requires an exact file structure like we have here\n",
        "# train_ds = datasets.ImageFolder(path/\"train\", transform=train_trans)\n",
        "# valid_ds = datasets.ImageFolder(path/\"val\", transform=valid_trans)\n",
        "\n",
        "\n",
        "# We could instead use the custom dataset we created\n",
        "# ~1min to process the data\n",
        "\n",
        "train_ds = CustomDogDataset(path, \"train\", train_trans)\n",
        "valid_ds = CustomDogDataset(path, \"val\", valid_trans)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Yznn9Rfd84v"
      },
      "source": [
        "# Make dataloaders\n",
        "bs = 16\n",
        "nw = 16\n",
        "train_loader = DataLoader(train_ds, batch_size=bs, shuffle=True, num_workers=nw)\n",
        "valid_loader = DataLoader(valid_ds, batch_size=bs, shuffle=False, num_workers=nw)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8kDN7g4LavvP"
      },
      "source": [
        "# Training loops\n",
        "\n",
        "Pretty much the same as last workshop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zt6NnLFGje97"
      },
      "source": [
        "def train(model, train_loader, loss_fn, optimizer, device):\n",
        "    # Prepare for training\n",
        "    model.train()\n",
        "    running_loss = 0\n",
        "  \n",
        "    with tqdm(total=len(train_loader)) as pbar:\n",
        "        for inputs, labels in train_loader:\n",
        "            # Put images and labels on GPU\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # Run through model and update\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = loss_fn(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()          \n",
        "\n",
        "            # Track loss and update progress\n",
        "            running_loss += loss.item()\n",
        "            pbar.update(1)\n",
        "\n",
        "    return running_loss / len(train_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tGoXkGRk8dz"
      },
      "source": [
        "# Function for the validation pass\n",
        "def validation(model, val_loader, loss_fn, device):\n",
        "    # Prepare for validating\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        with tqdm(total=len(val_loader)) as pbar:\n",
        "            for images, labels in iter(val_loader):\n",
        "                # Put images and labels on GPU\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "                # Run through model\n",
        "                outputs = model(images)\n",
        "\n",
        "                # Track loss and update progress\n",
        "                val_loss += loss_fn(outputs, labels).item()\n",
        "\n",
        "                # Update accuracy\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "                pbar.update(1)\n",
        "            \n",
        "    return val_loss / len(val_loader), correct / total"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHyzrQVPavvQ"
      },
      "source": [
        "# Writing a ResNet\n",
        "Rather than writing one giant class that handles every thing, we can write more readable code that is easier to maintain by breaking the model up into several smaller classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBZzu-CravvR"
      },
      "source": [
        "Every time that we use a convolution, we usually want to activate it with `BatchNorm` and `ReLU`. A ResNet is also composed mostly of 3x3 convolutions that maintain the input dimensions (`ks=3`, `stride=1` and `pad=1`).\n",
        "\n",
        "### Exercise: Complete the `Conv3x3` class to run a convolution, then optionally `BatchNorm` and `ReLU`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFx4Blu2k9e-"
      },
      "source": [
        "# Convolution, batch norm, ReLu with typical parameters defaulted\n",
        "class Conv(nn.Module):\n",
        "    def __init__(self, in_planes, out_planes, ks=3, stride=1, pad=1, bn=True, activ=True):\n",
        "        super().__init__()\n",
        "        # The main convolution\n",
        "        # Use nn.Conv2d to define a convolution layer with the parameters specified\n",
        "\n",
        "        # Optionally include activations\n",
        "        # If BatchNorm is True, use nn.BatchNorm2d to include a Batch Normalisation\n",
        "\n",
        "        # If activ is True, use nn.ReLU to include a ReLU\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Run through convolution then BatchNorm then ReLU\n",
        "\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTInR0wCavvS"
      },
      "source": [
        "![alt text](https://mohitjainweb.files.wordpress.com/2018/06/bottleneck-layer-resnet1.png?w=700)\n",
        "\n",
        "A ResNet uses one of two types of repeating blocks, the `Basic Block` and the `Bottleneck Block`. The Basic Block is simpler and simply has two 3x3 convolutions and a skip connection, it is used in ResNet 18 and ResNet 34. The Bottleneck Block is uses a 1x1 convolution .\n",
        "\n",
        "These blocks can also be used to downsample by increasing the stride to 2 on the first 3x3 convolution and adding a single 3x3 convolution to 'fix' the dimensions on the skip connection.\n",
        "\n",
        "### Exercise: Using the `Conv` class we've just made, complete the `Basic Block` class to run a double convolution with a skip connection. You can use the Bottleneck class as an example if you need to."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "STJ-5QBHlbM2"
      },
      "source": [
        "# ResNet Basic Block\n",
        "class BasicBlock(nn.Module):\n",
        "    def __init__(self, in_planes, out_planes, downsample=False):\n",
        "        super(BasicBlock, self).__init__()\n",
        "\n",
        "        # If downsample is True, we will downsample with a stride 2 convolution\n",
        "        self.downsample = downsample\n",
        "        if self.downsample:\n",
        "            # Use the Conv class to downsample, changing the number of filters to out_planes\n",
        "            \n",
        "            # We will also need a convolution to downsample on the skip connection to 'fix' the dimensions\n",
        "            # Use the Conv class to downsample without ReLU\n",
        "        \n",
        "        # If downsample is False, we can just use the default settings of our Conv class\n",
        "        else:\n",
        "            # Use the Conv class, changing the number of filters to out_planes\n",
        "        \n",
        "        # The second convolution doesn't use ReLU since this is applied after the skip connection\n",
        "        # Use the Conv class without ReLU\n",
        "        # Define a ReLU activation\n",
        "\n",
        "    def forward(self, x):\n",
        "        # If downsample, then save orignial \n",
        "\n",
        "        # Double convolution\n",
        "        \n",
        "        # Apply skip connection and final activation\n",
        "\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDOmB_Zxn7vy"
      },
      "source": [
        "# ResNet Bottleneck Block\n",
        "class Bottleneck(nn.Module):\n",
        "    def __init__(self, in_planes, out_planes, downsample=False):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        \n",
        "        # Reduce planes in the centre of the bottleneck by a factor of 4\n",
        "        reduced_planes = out_planes // 4\n",
        "        \n",
        "        # Reduce planes with a 1x1 convolution\n",
        "        self.conv1 = Conv(in_planes, reduced_planes, ks=1, pad=0)\n",
        "        \n",
        "        # Downsampling uses a stride 2 conv\n",
        "        self.downsample = downsample\n",
        "        \n",
        "        # If downsample is True, we will downsample with a stride 2 convolution\n",
        "        if self.downsample:\n",
        "            self.conv2 = Conv(reduced_planes, reduced_planes, stride=2, pad=2)\n",
        "            \n",
        "            # We will also need a convolution to downsample on the skip connection\n",
        "            self.downsample = Conv(in_planes, out_planes, ks=1, stride=2, activ=None)\n",
        "        \n",
        "        # If downsample is False, we can just use the default settings of our Conv class\n",
        "        else:\n",
        "            self.conv2 = Conv(reduced_planes, reduced_planes)\n",
        "        \n",
        "        # Increase planes with a 1x1 convolution\n",
        "        self.conv3 = Conv(reduced_planes, out_planes, ks=1, pad=0, activ=None)\n",
        "\n",
        "        \n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Save original or downsample\n",
        "        identity = self.downsample(x) if self.downsample else x\n",
        "\n",
        "        # Triple convolution\n",
        "        out = self.conv1(x)\n",
        "        out = self.conv2(out)\n",
        "        out = self.conv3(out)\n",
        "        \n",
        "        # Apply skip connection and final activation\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAspAAZDavvV"
      },
      "source": [
        "<img src=https://cdn-images-1.medium.com/max/2400/1*6hF97Upuqg_LdsqWY6n_wg.png style=\"margin-bottom: -270px;\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8Lx6zJMavvW"
      },
      "source": [
        "A `ResNet` can be decomposed into its input transition, the middle layers and its output transition:\n",
        "- The input transition uses a 7x7 convolution and a 3x3 max pool with a stride of 2. \n",
        "- The four 'middle layers' use repeating blocks with the first convolution downsampling. Though, since the input transition downsamples, the first middle layer will not downsample.\n",
        "- The output transition uses an average pool to take the average of each filter, then flattens the collected features to remove excess dimensions and passes this into a fully connected network\n",
        "\n",
        "### Exercise: Complete the `ResNet` class below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNJ6kI2llbss"
      },
      "source": [
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, depths, num_classes):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.block = block\n",
        "        self.depths = depths\n",
        "\n",
        "        # Input transition\n",
        "\n",
        "        # Downsample path\n",
        "        self.down1 = self._make_layer(64, 64, depths[0], downsample=False)\n",
        "        self.down2 = self._make_layer(64, 128, depths[1])\n",
        "        self.down3 = self._make_layer(128, 256, depths[2])\n",
        "        self.down4 = self._make_layer(256, 512, depths[3])\n",
        "\n",
        "        # Output transition\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.flatten = nn.Flatten(1)\n",
        "        self.fc = nn.Linear(in_features=512, out_features=num_classes)\n",
        "\n",
        "    # Create a middle layer\n",
        "    def _make_layer(self, in_channels, out_channels, depth, downsample=True):\n",
        "        # Increase the number of filters\n",
        "        layers = [# Change the filters and downsample with the first block]\n",
        "        \n",
        "        # Add a repeat the block depth - 1 times\n",
        "        for _ in range(depth - 1):\n",
        "            layers.append(# Repeat the block without changing the filters)\n",
        "        \n",
        "        # Convert the layers list into a Sequential\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Input\n",
        "\n",
        "        # Downsample\n",
        "\n",
        "        # Output\n",
        "\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vyJ7D6-sMKQ"
      },
      "source": [
        "# Define a series of functions to make each ResNet\n",
        "def resnet18(num_classes): return ResNet(BasicBlock, [2, 2, 2, 2], num_classes)\n",
        "def resnet34(num_classes): return ResNet(BasicBlock, [3, 4, 6, 3], num_classes)\n",
        "def resnet50(num_classes): return ResNet(Bottleneck, [3, 4, 6, 3], num_classes)\n",
        "def resnet101(num_classes): return ResNet(Bottleneck, [3, 4, 23, 3], num_classes)\n",
        "def resnet152(num_classes): return ResNet(Bottleneck, [3, 8, 36, 3], num_classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yD_5HFjkavvY"
      },
      "source": [
        "Compare our ResNet with PyTorch's ResNet. We've broken down the class differently and used different layer names, but the model's structure should be the same."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DvgnYZijavvY"
      },
      "source": [
        "resnet18(1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "HVdallaAavvZ"
      },
      "source": [
        "models.resnet18()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFxZNGyCavva"
      },
      "source": [
        "# Testing the Model\n",
        "Finally let's train our model to see how it goes. Feel free to experiment with the other ResNets to see how they differ in performance. If everything has worked as intended, it should approach the performance of a ResNet without pretraining."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fkb50ehaxCIF"
      },
      "source": [
        "model = resnet34(10).to(device)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0003)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJden8fZti7l"
      },
      "source": [
        "total_epoch = 20\n",
        "\n",
        "for epoch in range(total_epoch):\n",
        "    # Train the model for one epoch\n",
        "    train_loss = train(model, train_loader, loss_fn, optimizer, device)\n",
        "    # Calculate validation metrics for one epoch\n",
        "    val_loss, accuracy = validation(model, valid_loader, loss_fn, device)\n",
        "    print(\"Epoch: {}/{}, Training Loss: {:.4f}, Val Loss: {:.4f}, Val Accuracy: {:.4f}\".format(epoch+1, total_epoch, train_loss, val_loss, accuracy))\n",
        "    print('-' * 20)\n",
        "\n",
        "print(\"Finished Training\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_uMoxxDavvb"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fV3O7SJxavvc"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}