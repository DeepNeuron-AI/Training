{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Workshop 3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7ckYLzk1qXx"
      },
      "source": [
        "# Workshop 3 - Pytorch Model Creation\n",
        "DeepNeuron summer training 2020.\n",
        "\n",
        "Create a model using Pytorch which acts as a classifier for the CIFAR-10 dataset\n",
        "\n",
        "**Before starting:**\n",
        "\n",
        "1. **Don't edit this file, make a copy first:**\n",
        "  * Click on File -> Save a copy in Drive\n",
        "\n",
        "2. Also do the following:\n",
        "  * Click on Runtime -> Change runtime type -> Make sure hardware accelerator is set to GPU\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8A5h_HSa1ntg"
      },
      "source": [
        "## Imports\n",
        "Do all your imports here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EyssqirK0o_i"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "from tqdm.notebook import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YuT7EHsP0w6V"
      },
      "source": [
        "## Model Creation\n",
        "Create your model here. The model stub has already been created, you will need to define an `__init__` and a `forward()` method for your class.\n",
        "\n",
        "Your model will need:\n",
        "1. Convolutional Layer (3 input channels, 7 output channels, kernel size = 5)\n",
        "2. Max Pool (kernel size = 2, stride = 2)\n",
        "3. Convolutional Layer (7 input channels, 14 output channels, kernel size = 5)\n",
        "4. Fully Connected Layer (Figure out the input size, output size is 64)\n",
        "5. Fully Connected Layer (input size is 64, output size is the number of classes)\n",
        "\n",
        "Then, in your forward pass, the input should flow like so:\n",
        "1. Convolutional Layer 1\n",
        "2. ReLU\n",
        "3. Max Pool\n",
        "4. Convolutional Layer 2\n",
        "5. ReLU\n",
        "6. Max Pool (use the same max pool layer)\n",
        "7. Flatten the input, so it can be passed through the fully connected layers\n",
        "8. Fully Connected Layer 1\n",
        "9. ReLU\n",
        "10. Fully Connected Layer 2\n",
        "\n",
        "### Hints\n",
        "\n",
        "All layers are found in nn:\n",
        "* Fully Connected Layer: `Linear(num inputs, num outputs)`\n",
        "* Max Pooling: `MaxPool2d(kernel size, stride)`\n",
        "* Convolutional Layer: `Conv2d(input channels, output channels, kernel size, stride)`\n",
        "* Define your layers in the `__init__` method of your model\n",
        "* Flatten a PyTorch tensor using `.flatten()`\n",
        "\n",
        "The ReLU function can be found in `nn.functional.relu()`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PEhHODf0hyp"
      },
      "source": [
        "class MyCNN(nn.Module):\n",
        "  def __init__(self, output_size):\n",
        "      pass\n",
        "\n",
        "  def forward(self, x):\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4Af1zPc11cQ"
      },
      "source": [
        "## Training Using Your Model\n",
        "It's time to train your model!\n",
        "\n",
        "We use a basic PyTorch training loop, with standard built-in datasets, dataloaders and training loops"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKikAPOX8OUP"
      },
      "source": [
        "# Function for the training \n",
        "\n",
        "def train(model, train_loader, loss_fn, optimizer, device):\n",
        "    model.train() # puts the model in training mode\n",
        "    running_loss = 0\n",
        "    with tqdm(total=len(train_loader)) as pbar:\n",
        "        for i, data in enumerate(train_loader, 0): # loops through training data\n",
        "            inputs, labels = data # separate inputs and labels (outputs)\n",
        "            inputs, labels = inputs.to(device), labels.to(device) # puts the data on the GPU\n",
        "\n",
        "            # forward + backward + optimize                                          \n",
        "            optimizer.zero_grad() # clear the gradients in model parameters\n",
        "            outputs = model(inputs) # forward pass and get predictions\n",
        "            loss = loss_fn(outputs, labels) # calculate loss\n",
        "            loss.backward() # calculates gradient w.r.t to loss for all parameters in model that have requires_grad=True\n",
        "            optimizer.step() # iterate over all parameters in the model with requires_grad=True and update their weights.\n",
        "\n",
        "            running_loss += loss.item() # sum total loss in current epoch for print later\n",
        "\n",
        "            pbar.update(1) #increment our progress bar\n",
        "\n",
        "    return running_loss/len(train_loader) # returns the total training loss for the epoch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0aOkgss8Q9L"
      },
      "source": [
        "# Function for the validation pass\n",
        "\n",
        "def validation(model, val_loader, loss_fn, device):\n",
        "    model.eval() # puts the model in validation mode\n",
        "    running_loss = 0\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    \n",
        "    with torch.no_grad(): # save memory by not saving gradients which we don't need \n",
        "        with tqdm(total=len(val_loader)) as pbar:\n",
        "            for images, labels in iter(val_loader):\n",
        "                images, labels = images.to(device), labels.to(device) # put the data on the GPU\n",
        "                outputs = model(images) # passes image to the model, and gets a ouput which is the class probability prediction\n",
        "\n",
        "                val_loss = loss_fn(outputs, labels) # calculates val_loss from model predictions and true labels\n",
        "                running_loss += val_loss.item()\n",
        "                _, predicted = torch.max(outputs, 1) # turns class probability predictions to class labels\n",
        "                total += labels.size(0) # sums the number of predictions\n",
        "                correct += (predicted == labels).sum().item() # sums the number of correct predictions\n",
        "        \n",
        "                pbar.update(1)\n",
        "\n",
        "        return running_loss/len(val_loader), correct/total # return loss value, accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4SqDdINODxlI"
      },
      "source": [
        "## Dataset\n",
        "Set a path for the dataset downloads"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSU0v-izbdSw"
      },
      "source": [
        "train_path = 'data/train'\n",
        "valid_path = 'data/valid'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tC050ZXb7xST"
      },
      "source": [
        "# Define transforms for the training and validation set\n",
        "training_transforms = transforms.Compose([transforms.RandomRotation(30),\n",
        "                                          transforms.RandomHorizontalFlip(),\n",
        "                                          transforms.ToTensor(),\n",
        "                                          transforms.Normalize([0.485, 0.456, 0.406], \n",
        "                                                               [0.229, 0.224, 0.225])])\n",
        "\n",
        "validation_transforms = transforms.Compose([transforms.ToTensor(),\n",
        "                                            transforms.Normalize([0.485, 0.456, 0.406], \n",
        "                                                                 [0.229, 0.224, 0.225])])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbt08-r472CL"
      },
      "source": [
        "training_dataset = datasets.CIFAR10(train_path, train=True, transform=training_transforms, download=True)\n",
        "validation_dataset = datasets.CIFAR10(valid_path, train=False, transform=validation_transforms, download=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vWeIDeU175q_"
      },
      "source": [
        "training_loader = torch.utils.data.DataLoader(training_dataset, batch_size=32, shuffle=True)\n",
        "validation_loader = torch.utils.data.DataLoader(validation_dataset, batch_size=32, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NOPg6xkq76Ds"
      },
      "source": [
        "# Check what classes are in our dataset\n",
        "\n",
        "training_dataset.classes, validation_dataset.classes\n",
        "\n",
        "num_classes = len(training_dataset.classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydV1hGkjD4un"
      },
      "source": [
        "## Model Instantiation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJCMl2am7-dH"
      },
      "source": [
        "model = MyCNN(num_classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqxakZM7D-lc"
      },
      "source": [
        "We always need to keep track of where our PyTorch tensors are being kept i.e. whether or not they're on the GPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPRa5hVQ8Vg1"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") # Determine whether a GPU is available\n",
        "model.to(device) # send model to GPU"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5I9pnrkfGDuL"
      },
      "source": [
        "We now need to define our loss function and optimiser"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmD0vAxB8YaJ"
      },
      "source": [
        "loss_fn = nn.CrossEntropyLoss() # We use Cross Entropy Loss, as this is a classification task\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001) # If in doubt, we use Adam as our optimiser"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkigxKp1Ev7O"
      },
      "source": [
        "## Training Time!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-eMOAEs8bo7"
      },
      "source": [
        "total_epoch = 10 # Define how many epochs of training we want\n",
        "\n",
        "# keep track of things we'd like to plot later\n",
        "training_losses = []\n",
        "validation_losses = []\n",
        "accuracies = []\n",
        "\n",
        "for epoch in range(total_epoch): # loops through number of epochs\n",
        "  train_loss = train(model, training_loader, loss_fn, optimizer, device)  # train the model for one epoch\n",
        "  val_loss, accuracy = validation(model, validation_loader, loss_fn, device) # after training for one epoch, run the validation() function to see how the model is doing on the validation dataset\n",
        "  \n",
        "  # keep track of interesting stuff\n",
        "  training_losses.append(train_loss)\n",
        "  validation_losses.append(val_loss)\n",
        "  accuracies.append(accuracy)\n",
        "  \n",
        "  print(\"Epoch: {}/{}, Training Loss: {}, Val Loss: {}, Val Accuracy: {}\".format(epoch+1, total_epoch, train_loss, val_loss, accuracy))\n",
        "  print('-' * 20)\n",
        "\n",
        "print(\"Finished Training\")\n",
        "\n",
        "# Save the queen\n",
        "torch.save(model.state_dict(), 'finished')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OC8zCt2__uOi"
      },
      "source": [
        "We now want to visualise our results:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oaoHFQ4M_j_I"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(training_losses, label=\"Training Losses\")\n",
        "plt.plot(validation_losses, label=\"Validation Losses\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Loss vs Epoch\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.plot(accuracies, label=\"Accuracy\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Accuracy vs Epoch\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVhJxi3hT_51"
      },
      "source": [
        "## Once you're done...\n",
        "Even though the accuracy increases and the loss decreases, our model is not very good. This is normal. Try to improve your accuracy! Things you can change:\n",
        "- Learning rate\n",
        "- Number of epochs of training\n",
        "- Batch size\n",
        "- Different transforms\n",
        "- Model structure (number of layers, convolutional layer properties, new layer types like Dropout)\n",
        "\n",
        "Ultimately, it takes a lot of experimentation and gut feel to go from a basic training loop to an optimised model. There are tools to optimise these hyperparameters, but its always useful to be able to know a good place to start. As with most things, practice makes perfect. "
      ]
    }
  ]
}